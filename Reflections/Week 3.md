### Week 3 Reflection: Information Theory and Randomness

#### Intro

Week 3 was a bit of a blur since I was under the weather and barely hanging on during class. My brain was foggy, so I didn’t catch much live, but the notes and a quick skim afterward gave me a glimpse of information theory and randomness. Coming off the high of relationships and chaos theory from Weeks 1 and 2, I’m bummed I couldn’t fully geek out, but I’m piecing it together now. 

#### Takeaways

From what I could gather, Week 3 was about Claude Shannon’s information theory, which is about quantifying information and uncertainty using entropy. The idea that low-probability events (like a random road crash) carry more information because they’re surprising kind of like counterintuitive at first, but also made sense, because if I already know the outcome, then there's only so much information I could gather from that outcome. The Shannon entropy formula, ( H(X) = -\sum P(x) \log_2 P(x) ), measures unpredictability, where high entropy means more randomness (like a busy intersection) and low entropy means order (like a clear highway).

The notes on randomness and stochasticity also stood out. Adding noise to models, like random walks, helps them adapt and avoid getting stuck, which feels like a metaphor for life. SOmetime, you just have to make a shout it out with a loud voice to keep yourself moving forward. Haha. The coding examples, like calculating entropy for text strings (“aaaaaa” = no entropy, “abcdef” = high entropy), showed how patterns affect information. I wish I’d been alert enough to try it myself, but it’s sparking ideas for my interests.

#### Highlight

I guess the highlight for me was during that random walk simulations. Central Limit Theorem was again haunting me. I knew I'd at least meet it once for the Applied COmputational Statistics class a couple of terms ago, but I didn't realize it will come back again. You see, I have a love-hate relationship with CLT since my undergraduate days. But yeah, this example that says if I take multiple random walks, the average would always be close to where it started. So if there were a multiverse and there were multiple Denvers, the average of all the choices of those Denvers would be the Denver that Denver is expected to be. So I guess that's what fate is? That went philosophical real quick. Hahaha.

#### Other Thoughts

Even half-out-of-it, I can somehow see connections to my interests. For road safety, entropy could help model high-risk zones, like intersections with erratic driver patterns, and flag them for better signage or traffic control. My kink community idea also fits with strict rules like consent create low-entropy order, but new emerging subcultures pop up with high-entropy surprises. The random walk stuff makes me think of how trends spread in social groups, which could be a fun angle for either project.

#### Conclusion

Week 3 was tough to absorb while feeling like a zombie, but the notes on information theory and randomness still got me excited. Entropy and stochasticity feel like tools to decode the world’s messiness, from road hazards to subculture evolution. I’m bummed I couldn't fully catch up with the live coding, but every topic so far is exciting. Hoping to be back to full nerd mode by Week 4!