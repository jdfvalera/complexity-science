{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22e4340",
   "metadata": {},
   "source": [
    "# Information, Order, and Randomness Activity\n",
    "In this notebook, we'll explore how entropy and randomness reveal patterns (or the lack of them) in text and data.\n",
    "Specifically, we will:\n",
    "- Calculate the entropy of text sequence\n",
    "- Simulate and visualize a random walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb5e75",
   "metadata": {},
   "source": [
    "## Entropy of Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2e0bd-7afd-4624-b414-5100b7eb94cd",
   "metadata": {},
   "source": [
    "As discussed, entropy can be used as a measure of information in our data. In this case, we're simplifying it by looking at repeated characters in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "321d92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24256b12-61c1-4b54-9864-26c6016809c2",
   "metadata": {},
   "source": [
    "To compute for the entropy, we will use the equation:\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} P(x_i) \\log_2 P(x_i)\n",
    "$$\n",
    "\n",
    "- Measured in **bits** (if log base 2)\n",
    "- Higher entropy = more unpredictability\n",
    "- Lower entropy = more structure/predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c7a65b0-aa0c-4e89-8bc2-8b569d235806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(text):\n",
    "    freqs = Counter(text)\n",
    "    total = len(text)\n",
    "    probs = [f / total for f in freqs.values()]\n",
    "    return -sum(p * np.log2(p) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1797407-f5e8-4c80-991f-4705afb6f5b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:43:28.436617Z",
     "iopub.status.busy": "2025-04-15T23:43:28.435923Z",
     "iopub.status.idle": "2025-04-15T23:43:28.448065Z",
     "shell.execute_reply": "2025-04-15T23:43:28.445395Z",
     "shell.execute_reply.started": "2025-04-15T23:43:28.436561Z"
    }
   },
   "source": [
    "Let's try it on different sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810ab3f3-1c29-4090-92b8-6d368700f137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of text1: -0.0\n",
      "Entropy of text2: 2.584962500721156\n",
      "Entropy of text3: 2.0403733936884962\n"
     ]
    }
   ],
   "source": [
    "text1 = \"aaaaaa\"\n",
    "text2 = \"abcdef\"\n",
    "text3 = \"abracadabra\"\n",
    "\n",
    "print(\"Entropy of text1:\", shannon_entropy(text1))\n",
    "print(\"Entropy of text2:\", shannon_entropy(text2))\n",
    "print(\"Entropy of text3:\", shannon_entropy(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c133f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16b4f690-f70c-4b1d-819e-812d935e9aad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:44:27.358124Z",
     "iopub.status.busy": "2025-04-15T23:44:27.357358Z",
     "iopub.status.idle": "2025-04-15T23:44:27.367693Z",
     "shell.execute_reply": "2025-04-15T23:44:27.366309Z",
     "shell.execute_reply.started": "2025-04-15T23:44:27.358064Z"
    }
   },
   "source": [
    "#### Questions:\n",
    "1. How does the entropy change between the text samples?\n",
    "2. How do you think can this be applicable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69a271",
   "metadata": {},
   "source": [
    "#### Compression as a Proxy for Structure\n",
    "\n",
    "Note that we mentioned that entropy is  measure of information. We can verify this by looking at the compression size of the text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ceab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zlib\n",
    "\n",
    "def compressed_size(text):\n",
    "    return len(zlib.compress(text.encode()))\n",
    "\n",
    "print(\"Compressed size of text1:\", compressed_size(text1))\n",
    "print(\"Compressed size of text2:\", compressed_size(text2))\n",
    "print(\"Compressed size of text3:\", compressed_size(text3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850428b6-c294-46a9-bbef-471a74ab88ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T23:47:20.958017Z",
     "iopub.status.busy": "2025-04-15T23:47:20.957312Z",
     "iopub.status.idle": "2025-04-15T23:47:20.968660Z",
     "shell.execute_reply": "2025-04-15T23:47:20.966714Z",
     "shell.execute_reply.started": "2025-04-15T23:47:20.957958Z"
    }
   },
   "source": [
    "In this case, the higher the entropy, the higher the information, and the higher the final compressed size will be.\n",
    "\n",
    "How does this affect transmission of information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b4fa3-eca3-43fb-8270-399be47db77f",
   "metadata": {},
   "source": [
    "Note that entropy can be \"localized\" depending on the measurement window. Can you think of a use case where a system would have varying entropy across varying windows?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a53d3",
   "metadata": {},
   "source": [
    "## The Random Walk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c867aad-78a4-4990-ac65-8adfc29ba69a",
   "metadata": {},
   "source": [
    "In this section, we will simulate a random walk across one dimension. In this case, we either move up by one step or down by one step. We will then look at the long term behavior of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a406ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9817df19-b135-4be4-be09-66c2315eb35b",
   "metadata": {},
   "source": [
    "Let's first simulate one random walker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0333850-01c1-443f-b4aa-d41b76bd3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000\n",
    "x = [0]\n",
    "for _ in range(steps):\n",
    "    move = random.choice([-1, 1])\n",
    "    x.append(x[-1] + move)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(x)\n",
    "plt.title(\"1D Random Walk\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e6f49-c2f2-4f26-929e-f55ff108a36d",
   "metadata": {},
   "source": [
    "Try running the code multiple times and see how the patterns changes across runs.\n",
    "Specifcally, look at:\n",
    "1) Value of maximum position\n",
    "2) Final direction (positive or negative)\n",
    "\n",
    "What do you think is the long term behavior of this system? Let's try to implement a Monte Carlo simulation of different runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc1c228-f867-463c-b41c-6a5f54c03b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_walks = 500\n",
    "steps = 1000\n",
    "walks = np.zeros((num_walks, steps))\n",
    "\n",
    "for i in range(num_walks):\n",
    "    position = 0\n",
    "    for t in range(1, steps):\n",
    "        move = np.random.choice([-1, 1])\n",
    "        position += move\n",
    "        walks[i, t] = position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ae949-fe31-4684-b598-cb8ca9245696",
   "metadata": {},
   "source": [
    "Now let's look at two graphs, the standard deviation and the mean position over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a9bb1-d875-4066-b5c5-5e0d642ad4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = np.std(walks, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(std_dev, label='Standard Deviation')\n",
    "plt.title(\"Standard Deviation of Position Over Time Across Random Walks\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Standard Deviation\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e52b7-1360-434d-8810-2cab1a24395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_displacement = np.mean(walks, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(mean_displacement, label='Mean Displacement', color='orange')\n",
    "plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "plt.title(\"Mean Displacement Over Time Across Random Walks\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Mean Displacement\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca43fe2-ebb3-4ccd-9633-931a7436b6c7",
   "metadata": {},
   "source": [
    "How would you interpret the two plots?\n",
    "\n",
    "How do you think would this translate to a two-dimensional random walker?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86fdb31",
   "metadata": {},
   "source": [
    "### On Information and Randomness\n",
    "Note that in this notebook, we have demonstrated how we can quantify information and order via the entropy as well as look at how randomness can evolve over time. The idea is to integrate these topics into our subsequent models to capture possible emergent behavior. Order and randomness represent two ends of a spectrum that shape how we interpret, compress, and predict data.\n",
    "\n",
    "- Order allows us to find patterns, build structured models, and make accurate forecasts.\n",
    "- Randomness helps us capture uncertainty, variability, and noise â€” all of which are essential in real-world data.\n",
    "\n",
    "Understanding this balance:\n",
    "\n",
    "- Guides feature selection (which variables carry real signal?)\n",
    "- Shapes modeling decisions (linear vs probabilistic vs chaotic systems)\n",
    "\n",
    "Ultimately, learning to quantify and leverage both helps us build smarter, more robust, and more interpretable systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
